# #!/usr/bin/env python3
# """
# Script PySpark pour extraire, transformer et modÃ©liser un mini Data Warehouse
# Ã  partir de fichiers CSV stockÃ©s dans Hadoop HDFS et sauvegarde vers PostgreSQL.
# Gestion des clÃ©s primaires et Ã©trangÃ¨res selon les spÃ©cifications mÃ©tier.
# """

# from pyspark.sql import SparkSession
# from pyspark.sql.functions import (
#     col, trim, to_timestamp, concat, lit,
#     row_number, monotonically_increasing_id, round as spark_round,
#     coalesce, when, isnan, isnull, broadcast
# )
# from pyspark.sql.window import Window
# from pyspark.sql.types import StringType, IntegerType, DoubleType

# # -------------------------------
# # Configuration PostgreSQL
# # -------------------------------
# POSTGRES_CONFIG = {
#     "url": "jdbc:postgresql://localhost:5433/warehouse",
#     "driver": "org.postgresql.Driver",
#     "user": "dione",
#     "password": "Pass123"
# }

# # -------------------------------
# # Initialisation de la SparkSession avec driver PostgreSQL
# # -------------------------------
# spark = SparkSession.builder \
#     .appName("DataWarehouse_Transport_PostgreSQL") \
#     .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
#     .config("spark.sql.adaptive.enabled", "true") \
#     .config("spark.jars", "/path/to/postgresql-42.x.x.jar") \
#     .getOrCreate()

# spark.sparkContext.setLogLevel("WARN")

# # -------------------------------
# # DÃ©finition des fichiers utiles
# # -------------------------------
# files_to_load = {
#     "transport": "/data/mongodb_csv/OTO_transport.csv",
#     "passagers": "/data/mongodb_csv/Passagers.csv",
#     "gps": "/data/mongodb_csv/SEposition_GPS.csv",
#     "incident": "/data/mongodb_csv/incidents.csv",
#     "stops": "/data/mongodb_csv/stops.csv"
# }

# def read_csv(path):
#     """Lit un CSV depuis HDFS en DataFrame Spark"""
#     return spark.read.option("header", "true").option("inferSchema", "true").csv(path)

# def write_to_postgres(df, table_name, mode="overwrite"):
#     """Sauvegarde un DataFrame vers PostgreSQL"""
#     df.write \
#         .format("jdbc") \
#         .option("url", POSTGRES_CONFIG["url"]) \
#         .option("dbtable", table_name) \
#         .option("user", POSTGRES_CONFIG["user"]) \
#         .option("password", POSTGRES_CONFIG["password"]) \
#         .option("driver", POSTGRES_CONFIG["driver"]) \
#         .mode(mode) \
#         .save()

# # -------------------------------
# # Chargement des datasets
# # -------------------------------
# print("ğŸ“Š Chargement des datasets depuis HDFS...")
# df_transport = read_csv(files_to_load["transport"])
# df_passagers = read_csv(files_to_load["passagers"])
# df_gps       = read_csv(files_to_load["gps"])
# df_incident  = read_csv(files_to_load["incident"])
# df_stops     = read_csv(files_to_load["stops"])

# # -------------------------------
# # Fonctions utilitaires de nettoyage
# # -------------------------------
# def clean_and_handle_nulls(df, exclude_cols=None):
#     """
#     Nettoie un DataFrame : 
#     - Trim les colonnes string
#     - Remplace les valeurs nulles/vides par des valeurs par dÃ©faut
#     - Supprime les doublons
#     """
#     if exclude_cols is None:
#         exclude_cols = []
    
#     for c in df.columns:
#         if c not in exclude_cols:
#             if dict(df.dtypes)[c] == "string":
#                 df = df.withColumn(c, trim(col(c)))
#                 # Remplacer les chaÃ®nes vides par null puis par une valeur par dÃ©faut
#                 df = df.withColumn(c, when(col(c) == "", None).otherwise(col(c)))
#                 df = df.withColumn(c, coalesce(col(c), lit("Unknown")))
#             elif dict(df.dtypes)[c] in ("double", "float", "int", "bigint"):
#                 # Remplacer les valeurs nulles numÃ©riques par 0
#                 df = df.withColumn(c, coalesce(col(c), lit(0)))
    
#     return df.dropDuplicates()

# def round_numeric_columns(df, precision=2):
#     """Arrondit toutes les colonnes numÃ©riques Ã  la prÃ©cision spÃ©cifiÃ©e"""
#     for c, dtype in df.dtypes:
#         if dtype in ("double", "float"):
#             df = df.withColumn(c, spark_round(col(c), precision))
#     return df

# # -------------------------------
# # Transformation des datasets avec gestion des clÃ©s
# # -------------------------------

# print(" Nettoyage et transformation des datasets...")

# # 1. TRANSPORT : vehicle_id comme clÃ© primaire, suppression de _id
# print("   - Transformation OTO_transport.csv")
# df_transport_clean = clean_and_handle_nulls(df_transport)
# df_transport_clean = round_numeric_columns(df_transport_clean)

# # Supprimer _id et s'assurer que vehicle_id n'a pas de nulls
# if "_id" in df_transport_clean.columns:
#     df_transport_clean = df_transport_clean.drop("_id")

# # Filtrer les lignes oÃ¹ vehicle_id est null
# df_transport_clean = df_transport_clean.filter(col("vehicle_id").isNotNull())

# # 2. PASSAGERS : _id â†’ passager_id (pa_1, pa_2, ...), stop_id et vehicle_id comme clÃ©s Ã©trangÃ¨res
# print("   - Transformation Passagers.csv")
# df_passagers_clean = clean_and_handle_nulls(df_passagers)
# df_passagers_clean = round_numeric_columns(df_passagers_clean)

# # Supprimer _id et crÃ©er passager_id
# if "_id" in df_passagers_clean.columns:
#     df_passagers_clean = df_passagers_clean.drop("_id")

# # GÃ©nÃ©rer passager_id avec le format pa_1, pa_2, etc.
# window_passager = Window.orderBy(monotonically_increasing_id())
# df_passagers_clean = df_passagers_clean.withColumn(
#     "passager_id",
#     concat(lit("pa_"), row_number().over(window_passager))
# )

# # S'assurer que les clÃ©s Ã©trangÃ¨res ne sont pas nulles
# df_passagers_clean = df_passagers_clean.filter(
#     col("vehicle_id").isNotNull() & col("stop_id").isNotNull()
# )

# # 3. GPS : _id â†’ position_id (po_1, po_2, ...), vehicle_id comme clÃ© Ã©trangÃ¨re
# print("   - Transformation SEposition_GPS.csv")
# df_gps_clean = clean_and_handle_nulls(df_gps)
# df_gps_clean = round_numeric_columns(df_gps_clean)

# # Supprimer _id et crÃ©er position_id
# if "_id" in df_gps_clean.columns:
#     df_gps_clean = df_gps_clean.drop("_id")

# # GÃ©nÃ©rer position_id avec le format po_1, po_2, etc.
# window_gps = Window.orderBy(monotonically_increasing_id())
# df_gps_clean = df_gps_clean.withColumn(
#     "position_id",
#     concat(lit("po_"), row_number().over(window_gps))
# )

# # S'assurer que vehicle_id n'est pas null
# df_gps_clean = df_gps_clean.filter(col("vehicle_id").isNotNull())

# # 4. INCIDENTS : Incidents_id â†’ incident_id (clÃ© primaire), suppression de _id
# print("   - Transformation incidents.csv")
# df_incident_clean = clean_and_handle_nulls(df_incident)
# df_incident_clean = round_numeric_columns(df_incident_clean)

# # Renommer Incidents_id en incident_id et supprimer _id
# if "Incidents_id" in df_incident_clean.columns:
#     df_incident_clean = df_incident_clean.withColumnRenamed("Incidents_id", "incident_id")
# if "_id" in df_incident_clean.columns:
#     df_incident_clean = df_incident_clean.drop("_id")

# # S'assurer que les clÃ©s ne sont pas nulles
# df_incident_clean = df_incident_clean.filter(
#     col("incident_id").isNotNull() & col("vehicle_id").isNotNull()
# )

# # Normaliser les timestamps si prÃ©sents
# if "timestamp" in df_incident_clean.columns:
#     df_incident_clean = df_incident_clean.withColumn("timestamp", to_timestamp("timestamp"))

# # 5. STOPS : stop_id comme clÃ© primaire, ajout de vehicle_id et passager_id comme clÃ©s Ã©trangÃ¨res
# print("   - Transformation stops.csv")
# df_stops_clean = clean_and_handle_nulls(df_stops)
# df_stops_clean = round_numeric_columns(df_stops_clean)

# # Supprimer _id
# if "_id" in df_stops_clean.columns:
#     df_stops_clean = df_stops_clean.drop("_id")

# # S'assurer que stop_id n'est pas null
# df_stops_clean = df_stops_clean.filter(col("stop_id").isNotNull())

# # Ajouter vehicle_id depuis les vÃ©hicules existants dans transport
# vehicle_ids = df_transport_clean.select("vehicle_id").distinct().collect()
# vehicle_id_list = [row["vehicle_id"] for row in vehicle_ids]

# # CrÃ©er une relation stop -> vehicle_id (distribution cyclique pour exemple)
# if len(vehicle_id_list) > 0:
#     # Ajouter un index aux stops pour la distribution
#     window_stops = Window.orderBy("stop_id")
#     df_stops_with_index = df_stops_clean.withColumn(
#         "row_index", row_number().over(window_stops) - 1
#     )
    
#     # Distribution cyclique des vehicle_id
#     from pyspark.sql.functions import array, lit as spark_lit, element_at
#     vehicle_array = array([spark_lit(vid) for vid in vehicle_id_list])
#     df_stops_clean = df_stops_with_index.withColumn(
#         "vehicle_id",
#         element_at(vehicle_array, (col("row_index") % len(vehicle_id_list)) + 1)
#     ).drop("row_index")

# # Ajouter passager_id depuis les passagers existants (relation logique via stop_id)
# passager_stop_mapping = df_passagers_clean.select("passager_id", "stop_id").distinct()
# df_stops_clean = df_stops_clean.join(
#     passager_stop_mapping, "stop_id", "left"
# )

# # -------------------------------
# # CrÃ©ation des tables de dimensions
# # -------------------------------
# print("  CrÃ©ation des tables de dimensions...")

# # DIM_TRANSPORT : vehicle_id comme clÃ© primaire
# dim_transport = df_transport_clean.select(
#     "vehicle_id", "capacity", "company_name", "fuel_type", "status", "type_transport"
# )

# # DIM_PASSAGER : passager_id comme clÃ© primaire
# dim_passager = df_passagers_clean.select(
#     "passager_id", "alighting", "boarding", "passenger_count", 
#     "stop_id", "timestamp", "vehicle_id"
# )

# # DIM_GPS : position_id comme clÃ© primaire
# dim_gps = df_gps_clean.select(
#     "position_id", "latitude", "longitude", "route_id", 
#     "speed_kmh", "timestamp", "traffic_level", "vehicle_id"
# )

# # DIM_INCIDENT : incident_id comme clÃ© primaire
# dim_incident = df_incident_clean.select(
#     "incident_id", "delay_minutes", "description", 
#     "severity", "timestamp", "vehicle_id"
# )

# # DIM_STOP : stop_id comme clÃ© primaire, avec vehicle_id et passager_id comme clÃ©s Ã©trangÃ¨res
# dim_stop = df_stops_clean.select(
#     "stop_id", "latitude", "longitude", "name", 
#     "shelter", "zone", "vehicle_id", "passager_id"
# ).withColumn("passager_id", coalesce(col("passager_id"), lit("0")))

# # -------------------------------
# # CrÃ©ation de la table de faits
# # -------------------------------
# print(" CrÃ©ation de la table de faits...")

# # FACT_TRANSPORT : Consolidation de toutes les informations
# fact_transport = dim_transport

# # Jointure avec les passagers (agrÃ©gation par vÃ©hicule)
# passager_agg = dim_passager.groupBy("vehicle_id").agg(
#     {"passenger_count": "sum", "passager_id": "count"}
# ).withColumnRenamed("sum(passenger_count)", "total_passengers") \
#  .withColumnRenamed("count(passager_id)", "passenger_records")

# fact_transport = fact_transport.join(
#     broadcast(passager_agg), "vehicle_id", "left"
# )

# # Jointure avec les arrÃªts (via vehicle_id)
# stop_agg = dim_stop.groupBy("vehicle_id").agg(
#     {"stop_id": "count"}
# ).withColumnRenamed("count(stop_id)", "stops_count")

# fact_transport = fact_transport.join(
#     broadcast(stop_agg), "vehicle_id", "left"
# )

# # Jointure avec les incidents
# incident_agg = dim_incident.groupBy("vehicle_id").agg(
#     {"delay_minutes": "sum", "incident_id": "count"}
# ).withColumnRenamed("sum(delay_minutes)", "total_delay_minutes") \
#  .withColumnRenamed("count(incident_id)", "incident_count")

# fact_transport = fact_transport.join(
#     broadcast(incident_agg), "vehicle_id", "left"
# )

# # Jointure avec GPS (derniÃ¨re position par vÃ©hicule)
# gps_latest = dim_gps.withColumn(
#     "rn", row_number().over(Window.partitionBy("vehicle_id").orderBy(col("timestamp").desc()))
# ).filter(col("rn") == 1).drop("rn")

# fact_transport = fact_transport.join(
#     gps_latest.select("vehicle_id", "latitude", "longitude", "speed_kmh", "traffic_level"), 
#     "vehicle_id", "left"
# )

# # -------------------------------
# # Nettoyage final de la table de faits
# # -------------------------------
# print(" Nettoyage final et gestion des valeurs nulles...")

# fact_transport = fact_transport.withColumn(
#     "total_passengers", coalesce(col("total_passengers"), lit(0))
# ).withColumn(
#     "passenger_records", coalesce(col("passenger_records"), lit(0))
# ).withColumn(
#     "stops_count", coalesce(col("stops_count"), lit(0))
# ).withColumn(
#     "total_delay_minutes", coalesce(col("total_delay_minutes"), lit(0))
# ).withColumn(
#     "incident_count", coalesce(col("incident_count"), lit(0))
# ).withColumn(
#     "latitude", coalesce(col("latitude"), lit(0.0))
# ).withColumn(
#     "longitude", coalesce(col("longitude"), lit(0.0))
# ).withColumn(
#     "speed_kmh", coalesce(col("speed_kmh"), lit(0.0))
# ).withColumn(
#     "traffic_level", coalesce(col("traffic_level"), lit("Unknown"))
# )

# # Arrondir les valeurs numÃ©riques
# fact_transport = round_numeric_columns(fact_transport)

# # Filtrer les vÃ©hicules hors service pour Ã©viter les incohÃ©rences
# fact_transport = fact_transport.filter(col("status") != "hors service")

# # -------------------------------
# # Sauvegarde dans PostgreSQL
# # -------------------------------
# print(" Sauvegarde des tables dans PostgreSQL...")

# try:
#     # Sauvegarde des dimensions
#     print("   - Sauvegarde dim_transport...")
#     write_to_postgres(dim_transport, "dim_transport")
    
#     print("   - Sauvegarde dim_passager...")
#     write_to_postgres(dim_passager, "dim_passager")
    
#     print("   - Sauvegarde dim_gps...")
#     write_to_postgres(dim_gps, "dim_gps")
    
#     print("   - Sauvegarde dim_incident...")
#     write_to_postgres(dim_incident, "dim_incident")
    
#     print("   - Sauvegarde dim_stop...")
#     write_to_postgres(dim_stop, "dim_stop")
    
#     # Sauvegarde de la table de faits
#     print("   - Sauvegarde fact_transport...")
#     write_to_postgres(fact_transport, "fact_transport")
    
#     print(" Toutes les tables ont Ã©tÃ© sauvegardÃ©es avec succÃ¨s!")
    
# except Exception as e:
#     print(f" Erreur lors de la sauvegarde: {str(e)}")
#     print("VÃ©rifiez votre configuration PostgreSQL et la connexion rÃ©seau.")

# # -------------------------------
# # Affichage des statistiques
# # -------------------------------
# print("\n Statistiques du Data Warehouse:")
# print(f"   - VÃ©hicules de transport: {dim_transport.count()}")
# print(f"   - Passagers: {dim_passager.count()}")
# print(f"   - Positions GPS: {dim_gps.count()}")
# print(f"   - Incidents: {dim_incident.count()}")
# print(f"   - ArrÃªts: {dim_stop.count()}")
# print(f"   - Faits transport: {fact_transport.count()}")

# print("\n Data Warehouse gÃ©nÃ©rÃ© avec succÃ¨s dans PostgreSQL!")
# print(" Tables crÃ©Ã©es:")
# print("   - dim_transport (clÃ©: vehicle_id)")
# print("   - dim_passager (clÃ©: passager_id)")
# print("   - dim_gps (clÃ©: position_id)")
# print("   - dim_incident (clÃ©: incident_id)")
# print("   - dim_stop (clÃ©: stop_id)")
# print("   - fact_transport (table de faits centrale)")

# print("\n Pour vous connecter Ã  PostgreSQL et vÃ©rifier les donnÃ©es:")
# print(f"   psql -h localhost -p 5433 -U {POSTGRES_CONFIG['user']} -d warehouse")

# spark.stop()


#!/usr/bin/env python3
"""
Script PySpark pour extraire, transformer et modÃ©liser un mini Data Warehouse
Ã  partir de fichiers CSV stockÃ©s dans Hadoop HDFS et sauvegarde vers PostgreSQL.
Version amÃ©liorÃ©e avec meilleure gestion des erreurs et diagnostics.
"""

import psycopg2
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, trim, to_timestamp, concat, lit,
    row_number, monotonically_increasing_id, round as spark_round,
    coalesce, when, isnan, isnull, broadcast
)
from pyspark.sql.window import Window
from pyspark.sql.types import StringType, IntegerType, DoubleType
import traceback
import sys

# -------------------------------
# Configuration PostgreSQL avec diagnostics amÃ©liorÃ©s
# -------------------------------
def test_postgres_connection():
    """Test la connexion PostgreSQL avec diagnostics dÃ©taillÃ©s"""
    print("ğŸ” Test de connexion PostgreSQL...")
    
    # Test 1: Connexion de base
    try:
        conn = psycopg2.connect(
            host="localhost",
            port=5433,
            database="warehouse",
            user="dione", 
            password="Pass123"
        )
        print("âœ… Connexion PostgreSQL Ã©tablie")
        
        # Test 2: VÃ©rification de la base de donnÃ©es
        cursor = conn.cursor()
        cursor.execute("SELECT current_database(), current_user, version();")
        db_info = cursor.fetchone()
        print(f"ğŸ“Š Base: {db_info[0]}, Utilisateur: {db_info[1]}")
        print(f"ğŸ“‹ Version PostgreSQL: {db_info[2][:50]}...")
        
        # Test 3: Permissions d'Ã©criture
        cursor.execute("CREATE TABLE IF NOT EXISTS test_permissions (id INTEGER);")
        cursor.execute("DROP TABLE test_permissions;")
        conn.commit()
        print("âœ… Permissions d'Ã©criture vÃ©rifiÃ©es")
        
        cursor.close()
        return conn
        
    except psycopg2.OperationalError as e:
        print(f"âŒ Erreur de connexion PostgreSQL: {e}")
        print("ğŸ’¡ VÃ©rifiez:")
        print("   - PostgreSQL est dÃ©marrÃ©: sudo systemctl status postgresql")
        print("   - Port 5433 est ouvert: netstat -tlnp | grep 5433")
        print("   - Base 'warehouse' existe: psql -l")
        print("   - Utilisateur 'dione' a les permissions")
        return None
    except psycopg2.Error as e:
        print(f"âŒ Erreur PostgreSQL: {e}")
        return None
    except Exception as e:
        print(f"âŒ Erreur inattendue: {e}")
        print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
        return None

def get_postgres_connection():
    """Ã‰tablit une connexion PostgreSQL avec gestion d'erreurs"""
    return test_postgres_connection()

def create_tables_schema(conn):
    """CrÃ©e le schÃ©ma des tables dans PostgreSQL avec gestion d'erreurs"""
    cursor = conn.cursor()
    
    try:
        print("ğŸ—‘ï¸  Suppression des tables existantes...")
        tables_to_drop = [
            "fact_transport", "dim_transport", "dim_passager", 
            "dim_gps", "dim_incident", "dim_stop"
        ]
        
        for table in tables_to_drop:
            cursor.execute(f"DROP TABLE IF EXISTS {table} CASCADE;")
            print(f"   - Table {table} supprimÃ©e")
        
        print("ğŸ—ï¸  CrÃ©ation des nouvelles tables...")
        
        # CrÃ©er les tables de dimensions
        create_queries = {
            "dim_transport": """
                CREATE TABLE dim_transport (
                    vehicle_id VARCHAR(50) PRIMARY KEY,
                    capacity INTEGER,
                    company_name VARCHAR(100),
                    fuel_type VARCHAR(50),
                    status VARCHAR(50),
                    type_transport VARCHAR(50)
                );
            """,
            "dim_passager": """
                CREATE TABLE dim_passager (
                    passager_id VARCHAR(50) PRIMARY KEY,
                    alighting INTEGER,
                    boarding INTEGER,
                    passenger_count INTEGER,
                    stop_id VARCHAR(50),
                    timestamp TIMESTAMP,
                    vehicle_id VARCHAR(50)
                );
            """,
            "dim_gps": """
                CREATE TABLE dim_gps (
                    position_id VARCHAR(50) PRIMARY KEY,
                    latitude DECIMAL(10,8),
                    longitude DECIMAL(11,8),
                    route_id VARCHAR(50),
                    speed_kmh DECIMAL(5,2),
                    timestamp TIMESTAMP,
                    traffic_level VARCHAR(50),
                    vehicle_id VARCHAR(50)
                );
            """,
            "dim_incident": """
                CREATE TABLE dim_incident (
                    incident_id VARCHAR(50) PRIMARY KEY,
                    delay_minutes INTEGER,
                    description TEXT,
                    severity VARCHAR(50),
                    timestamp TIMESTAMP,
                    vehicle_id VARCHAR(50)
                );
            """,
            "dim_stop": """
                CREATE TABLE dim_stop (
                    stop_id VARCHAR(50) PRIMARY KEY,
                    latitude DECIMAL(10,8),
                    longitude DECIMAL(11,8),
                    name VARCHAR(100),
                    shelter BOOLEAN,
                    zone VARCHAR(50),
                    vehicle_id VARCHAR(50),
                    passager_id VARCHAR(50)
                );
            """,
            "fact_transport": """
                CREATE TABLE fact_transport (
                    vehicle_id VARCHAR(50) PRIMARY KEY,
                    capacity INTEGER,
                    company_name VARCHAR(100),
                    fuel_type VARCHAR(50),
                    status VARCHAR(50),
                    type_transport VARCHAR(50),
                    total_passengers INTEGER DEFAULT 0,
                    passenger_records INTEGER DEFAULT 0,
                    stops_count INTEGER DEFAULT 0,
                    total_delay_minutes INTEGER DEFAULT 0,
                    incident_count INTEGER DEFAULT 0,
                    latitude DECIMAL(10,8) DEFAULT 0,
                    longitude DECIMAL(11,8) DEFAULT 0,
                    speed_kmh DECIMAL(5,2) DEFAULT 0,
                    traffic_level VARCHAR(50) DEFAULT 'Unknown'
                );
            """
        }
        
        for table_name, query in create_queries.items():
            cursor.execute(query)
            print(f"   âœ… Table {table_name} crÃ©Ã©e")
        
        conn.commit()
        cursor.close()
        print("âœ… SchÃ©ma des tables crÃ©Ã© avec succÃ¨s!")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de la crÃ©ation du schÃ©ma: {e}")
        print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
        conn.rollback()
        cursor.close()
        return False

def dataframe_to_postgres(spark_df, table_name, conn):
    """Convertit un DataFrame Spark en pandas et l'insÃ¨re dans PostgreSQL avec diagnostics"""
    try:
        row_count = spark_df.count()
        print(f"ğŸ“ Traitement de {row_count} lignes pour {table_name}...")
        
        if row_count == 0:
            print(f"âš ï¸  Aucune donnÃ©e Ã  insÃ©rer dans {table_name}")
            return True
        
        # Afficher le schÃ©ma du DataFrame
        print(f"ğŸ“‹ SchÃ©ma de {table_name}:")
        spark_df.printSchema()
        
        # Afficher quelques exemples de donnÃ©es
        print(f"ğŸ” AperÃ§u des donnÃ©es de {table_name}:")
        spark_df.show(3, truncate=False)
        
        # Convertir Spark DataFrame en pandas
        print(f"ğŸ”„ Conversion Spark â†’ pandas pour {table_name}...")
        pandas_df = spark_df.toPandas()
        
        # Diagnostics sur les donnÃ©es pandas
        print(f"ğŸ“Š Dimensions pandas: {pandas_df.shape}")
        print(f"ğŸ“Š Colonnes: {list(pandas_df.columns)}")
        print(f"ğŸ“Š Types de donnÃ©es:\n{pandas_df.dtypes}")
        
        # Remplacer les valeurs NaN par None pour PostgreSQL
        pandas_df = pandas_df.where(pd.notnull(pandas_df), None)
        
        # Insertion dans PostgreSQL avec transaction
        cursor = conn.cursor()
        
        try:
            # VÃ©rifier que la table existe
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name,))
            
            if not cursor.fetchone()[0]:
                print(f"âŒ La table {table_name} n'existe pas!")
                return False
            
            # Construire la requÃªte d'insertion
            columns = list(pandas_df.columns)
            placeholders = ','.join(['%s'] * len(columns))
            columns_str = ','.join(columns)
            
            insert_query = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"
            print(f"ğŸ“ RequÃªte d'insertion: {insert_query}")
            
            # Insertion par batch pour de meilleures performances
            batch_size = 1000
            total_inserted = 0
            
            for i in range(0, len(pandas_df), batch_size):
                batch = pandas_df.iloc[i:i+batch_size]
                batch_data = [tuple(row) for row in batch.values]
                
                cursor.executemany(insert_query, batch_data)
                total_inserted += len(batch_data)
                print(f"   ğŸ“¦ Batch {i//batch_size + 1}: {len(batch_data)} lignes insÃ©rÃ©es")
            
            conn.commit()
            print(f"âœ… {total_inserted} lignes insÃ©rÃ©es dans {table_name}")
            
            # VÃ©rification post-insertion
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            db_count = cursor.fetchone()[0]
            print(f"ğŸ” VÃ©rification: {db_count} lignes dans la base")
            
            cursor.close()
            return True
            
        except Exception as insert_error:
            print(f"âŒ Erreur lors de l'insertion dans {table_name}: {insert_error}")
            print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
            conn.rollback()
            cursor.close()
            return False
        
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale pour {table_name}: {e}")
        print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
        return False

# -------------------------------
# Test de connectivitÃ© HDFS
# -------------------------------
def test_hdfs_connectivity():
    """Test la connectivitÃ© HDFS"""
    print("ğŸ” Test de connectivitÃ© HDFS...")
    
    try:
        # CrÃ©er une session Spark temporaire pour tester HDFS
        test_spark = SparkSession.builder \
            .appName("HDFS_Test") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
            .getOrCreate()
        
        # Test de lecture d'un rÃ©pertoire HDFS
        hadoop_conf = test_spark.sparkContext._jsc.hadoopConfiguration()
        fs = test_spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)
        
        # VÃ©rifier si le rÃ©pertoire /data existe
        path = test_spark.sparkContext._jvm.org.apache.hadoop.fs.Path("/data")
        if fs.exists(path):
            print("âœ… Connexion HDFS rÃ©ussie - rÃ©pertoire /data accessible")
            
            # Lister les fichiers dans /data
            files = fs.listStatus(path)
            print(f"ğŸ“ Contenu de /data: {len(files)} Ã©lÃ©ments")
            for file in files[:5]:  # Afficher les 5 premiers
                print(f"   - {file.getPath().getName()}")
        else:
            print("âš ï¸  RÃ©pertoire /data non trouvÃ© dans HDFS")
        
        test_spark.stop()
        return True
        
    except Exception as e:
        print(f"âŒ Erreur de connectivitÃ© HDFS: {e}")
        print("ğŸ’¡ VÃ©rifiez:")
        print("   - Hadoop est dÃ©marrÃ©: jps")
        print("   - HDFS est accessible: hdfs dfs -ls /")
        print("   - Le rÃ©pertoire /data existe: hdfs dfs -ls /data")
        if 'test_spark' in locals():
            test_spark.stop()
        return False

# -------------------------------
# Initialisation de la SparkSession avec gestion d'erreurs
# -------------------------------
def initialize_spark():
    """Initialise Spark avec gestion d'erreurs"""
    try:
        print("ğŸ”¥ Initialisation de Spark...")
        spark = SparkSession.builder \
            .appName("DataWarehouse_Transport_PostgreSQL") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        print("âœ… Spark initialisÃ© avec succÃ¨s")
        
        # Afficher les informations de la session
        print(f"ğŸ“Š Spark Version: {spark.version}")
        print(f"ğŸ“Š App Name: {spark.sparkContext.appName}")
        print(f"ğŸ“Š Master: {spark.sparkContext.master}")
        
        return spark
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'initialisation de Spark: {e}")
        print("ğŸ’¡ VÃ©rifiez que Spark est correctement installÃ© et configurÃ©")
        return None

# -------------------------------
# SCRIPT PRINCIPAL
# -------------------------------
def main():
    print("=" * 60)
    print("ğŸš€ DÃ‰MARRAGE DU SCRIPT DATA WAREHOUSE")
    print("=" * 60)
    
    # Test 1: ConnectivitÃ© HDFS
    if not test_hdfs_connectivity():
        print("âŒ Impossible de continuer sans HDFS. ArrÃªt du script.")
        return 1
    
    # Test 2: Initialisation Spark
    spark = initialize_spark()
    if spark is None:
        print("âŒ Impossible d'initialiser Spark. ArrÃªt du script.")
        return 1
    
    # Test 3: Connexion PostgreSQL
    conn = get_postgres_connection()
    if conn is None:
        print("âŒ Impossible de se connecter Ã  PostgreSQL. ArrÃªt du script.")
        spark.stop()
        return 1
    
    # Test 4: CrÃ©ation du schÃ©ma
    if not create_tables_schema(conn):
        print("âŒ Impossible de crÃ©er le schÃ©ma. ArrÃªt du script.")
        conn.close()
        spark.stop()
        return 1
    
    try:
        # DÃ©finition des fichiers
        files_to_load = {
            "transport": "/data/mongodb_csv/OTO_transport.csv",
            "passagers": "/data/mongodb_csv/Passagers.csv",
            "gps": "/data/mongodb_csv/SEposition_GPS.csv",
            "incident": "/data/mongodb_csv/incidents.csv",
            "stops": "/data/mongodb_csv/stops.csv"
        }
        
        def read_csv_with_validation(path, name):
            """Lit un CSV avec validation"""
            try:
                print(f"ğŸ“– Lecture de {name} depuis {path}...")
                df = spark.read.option("header", "true").option("inferSchema", "true").csv(path)
                count = df.count()
                print(f"âœ… {name}: {count} lignes chargÃ©es")
                if count > 0:
                    print(f"ğŸ“‹ Colonnes: {df.columns}")
                    df.show(2, truncate=False)
                return df
            except Exception as e:
                print(f"âŒ Erreur lors du chargement de {name}: {e}")
                return None
        
        # Chargement des datasets avec validation
        print("\nğŸ“Š CHARGEMENT DES DATASETS DEPUIS HDFS")
        print("-" * 50)
        
        datasets = {}
        for key, path in files_to_load.items():
            df = read_csv_with_validation(path, key)
            if df is None:
                print(f"âŒ Impossible de charger {key}. ArrÃªt du script.")
                conn.close()
                spark.stop()
                return 1
            datasets[key] = df
        
        # [Ici vous pouvez ajouter le reste de votre logique de transformation]
        # Pour l'instant, on teste juste l'insertion d'une table simple
        
        print("\nğŸ’¾ TEST D'INSERTION DANS POSTGRESQL")
        print("-" * 50)
        
        # Test avec la table transport
        transport_sample = datasets["transport"].limit(5)  # Juste 5 lignes pour test
        
        if dataframe_to_postgres(transport_sample, "dim_transport", conn):
            print("âœ… Test d'insertion rÃ©ussi!")
        else:
            print("âŒ Test d'insertion Ã©chouÃ©!")
        
        print("\nğŸ‰ SCRIPT TERMINÃ‰")
        
    except Exception as e:
        print(f"âŒ Erreur dans le script principal: {e}")
        print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
        return 1
        
    finally:
        # Nettoyage
        if 'conn' in locals() and conn:
            conn.close()
            print("ğŸ“ Connexion PostgreSQL fermÃ©e")
        
        if 'spark' in locals() and spark:
            spark.stop()
            print("ğŸ“ Session Spark fermÃ©e")
    
    return 0

# -------------------------------
# Point d'entrÃ©e
# -------------------------------
if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)