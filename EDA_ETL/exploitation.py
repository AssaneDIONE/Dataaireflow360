#!/usr/bin/env python3
"""
Script pour explorer et analyser le contenu des tables du Data Warehouse
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as spark_sum, avg, max as spark_max, min as spark_min

# Initialiser Spark
spark = SparkSession.builder \
    .appName("DataWarehouse_Explorer") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Chemin vers le warehouse
warehouse_path = "hdfs://localhost:9000/warehouse"

def analyze_table(table_name, df):
    """Analyse complète d'une table"""
    print(f"\n{'='*60}")
    print(f" ANALYSE DE LA TABLE : {table_name.upper()}")
    print(f"{'='*60}")
    
    # 1. Schéma de la table
    print(f"\n SCHÉMA DE LA TABLE :")
    df.printSchema()
    
    # 2. Nombre de lignes
    row_count = df.count()
    print(f"\n STATISTIQUES GÉNÉRALES :")
    print(f"   • Nombre de lignes : {row_count:,}")
    print(f"   • Nombre de colonnes : {len(df.columns)}")
    
    # 3. Aperçu des données
    print(f"\n APERÇU DES DONNÉES (40 premières lignes) :")
    df.show(40, truncate=False)
    
    # 4. Statistiques descriptives pour les colonnes numériques
    numeric_cols = [field.name for field in df.schema.fields 
                   if field.dataType.typeName() in ['integer', 'double', 'float', 'long']]
    
    if numeric_cols:
        print(f"\n STATISTIQUES NUMÉRIQUES :")
        df.select(numeric_cols).describe().show()
    
    # 5. Valeurs nulles par colonne
    print(f"\n VALEURS NULLES PAR COLONNE :")
    for col_name in df.columns:
        null_count = df.filter(col(col_name).isNull()).count()
        null_percentage = (null_count / row_count * 100) if row_count > 0 else 0
        print(f"   • {col_name}: {null_count:,} ({null_percentage:.2f}%)")
    
    # 6. Cardinalité (valeurs distinctes) pour colonnes string
    print(f"\n CARDINALITÉ (valeurs distinctes) :")
    for col_name in df.columns:
        if df.schema[col_name].dataType.typeName() == 'string':
            distinct_count = df.select(col_name).distinct().count()
            print(f"   • {col_name}: {distinct_count:,} valeurs distinctes")
    
    return row_count

def analyze_dimension_relationships():
    """Analyse les relations entre les dimensions"""
    print(f"\n{'='*80}")
    print(f" ANALYSE DES RELATIONS ENTRE TABLES")
    print(f"{'='*80}")
    
    # Charger les tables
    dim_passager = spark.read.parquet(f"{warehouse_path}/dim_passager")
    dim_transport = spark.read.parquet(f"{warehouse_path}/dim_transport") 
    dim_stop = spark.read.parquet(f"{warehouse_path}/dim_stop")
    dim_incident = spark.read.parquet(f"{warehouse_path}/dim_incident")
    fact_transport = spark.read.parquet(f"{warehouse_path}/fact_transport")
    
    print(f"\n RELATIONS PAR VEHICLE_ID :")
    vehicles_transport = dim_transport.select("vehicle_id").distinct().count()
    vehicles_passager = dim_passager.select("vehicle_id").distinct().count()
    vehicles_incident = dim_incident.select("vehicle_id").distinct().count()
    vehicles_fact = fact_transport.select("vehicle_id").distinct().count()
    
    print(f"   • Transport: {vehicles_transport:,} véhicules")
    print(f"   • Passagers: {vehicles_passager:,} véhicules") 
    print(f"   • Incidents: {vehicles_incident:,} véhicules")
    print(f"   • Table de faits: {vehicles_fact:,} véhicules")
    
    print(f"\n RELATIONS PAR STOP :")
    stops_dim = dim_stop.select("stop_id").distinct().count()
    stops_passager = dim_passager.select("stop_id").distinct().count()
    stops_fact = fact_transport.agg(spark_sum("stops_count").alias("total_stops")).collect()[0]["total_stops"]
    
    print(f"   • Dimension stops: {stops_dim:,} arrêts distincts")
    print(f"   • Dans passagers: {stops_passager:,} arrêts distincts")
    print(f"   • Dans table de faits: {stops_fact:,} arrêts enregistrés")

def analyze_business_metrics():
    """Analyse des métriques métier importantes"""
    print(f"\n{'='*80}")
    print(f" MÉTRIQUES MÉTIER CLÉS")
    print(f"{'='*80}")
    
    fact_transport = spark.read.parquet(f"{warehouse_path}/fact_transport")
    
    print(f"\n ANALYSE DES TRANSPORTS :")
    print("Répartition par type de transport :")
    fact_transport.groupBy("type_transport").count().orderBy("count", ascending=False).show()
    
    print("Top 10 des compagnies par nombre de véhicules :")
    fact_transport.groupBy("company_name").count().orderBy("count", ascending=False).show(10)
    
    print("Statistiques de capacité des véhicules :")
    fact_transport.select("capacity").describe().show()
    
    print(f"\n ANALYSE DES INCIDENTS :")
    incidents_stats = fact_transport.filter(col("incident_count").isNotNull()) \
        .agg(
            spark_sum("incident_count").alias("total_incidents"),
            avg("incident_count").alias("avg_incidents_per_vehicle"),
            spark_max("incident_count").alias("max_incidents"),
            spark_sum("total_delay_minutes").alias("total_delay"),
            avg("total_delay_minutes").alias("avg_delay_per_vehicle")
        ).collect()[0]
    
    print(f"   • Total incidents: {incidents_stats['total_incidents']}")
    print(f"   • Moyenne incidents/véhicule: {incidents_stats['avg_incidents_per_vehicle']:.2f}")
    print(f"   • Maximum incidents/véhicule: {incidents_stats['max_incidents']}")
    print(f"   • Retard total (minutes): {incidents_stats['total_delay']}")
    print(f"   • Retard moyen/véhicule: {incidents_stats['avg_delay_per_vehicle']:.2f} min")
    
    print(f"\n ANALYSE DES PASSAGERS :")
    passenger_stats = fact_transport.filter(col("total_passengers").isNotNull()) \
        .agg(
            spark_sum("total_passengers").alias("total_passengers"),
            avg("total_passengers").alias("avg_passengers"),
            spark_max("total_passengers").alias("max_passengers")
        ).collect()[0]
    
    print(f"   • Total passagers: {passenger_stats['total_passengers']}")
    print(f"   • Moyenne passagers/véhicule: {passenger_stats['avg_passengers']:.2f}")
    print(f"   • Maximum passagers/véhicule: {passenger_stats['max_passengers']}")

def main():
    """Fonction principale d'exploration"""
    print("  EXPLORATION COMPLÈTE DU DATA WAREHOUSE TRANSPORT")
    
    try:
        tables_info = {}
        
        print(f"\n ANALYSE DES TABLES DE DIMENSIONS")
        print(f"{'='*50}")
        
        dim_passager = spark.read.parquet(f"{warehouse_path}/dim_passager")
        tables_info['dim_passager'] = analyze_table("DIM_PASSAGER", dim_passager)
        
        dim_transport = spark.read.parquet(f"{warehouse_path}/dim_transport")
        tables_info['dim_transport'] = analyze_table("DIM_TRANSPORT", dim_transport)
        
        dim_stop = spark.read.parquet(f"{warehouse_path}/dim_stop") 
        tables_info['dim_stop'] = analyze_table("DIM_STOP", dim_stop)
        
        dim_incident = spark.read.parquet(f"{warehouse_path}/dim_incident")
        tables_info['dim_incident'] = analyze_table("DIM_INCIDENT", dim_incident)
        
        print(f"\n ANALYSE DE LA TABLE DE FAITS")
        print(f"{'='*40}")
        
        fact_transport = spark.read.parquet(f"{warehouse_path}/fact_transport")
        tables_info['fact_transport'] = analyze_table("FACT_TRANSPORT", fact_transport)
        
        analyze_dimension_relationships()
        analyze_business_metrics()
        
        print(f"\n{'='*80}")
        print(f" RÉSUMÉ EXÉCUTIF DU DATA WAREHOUSE")
        print(f"{'='*80}")
        
        total_rows = sum(tables_info.values())
        print(f"    Total lignes dans le warehouse: {total_rows:,}")
        print(f"    Répartition par table:")
        for table, rows in tables_info.items():
            percentage = (rows / total_rows * 100) if total_rows > 0 else 0
            print(f"      • {table}: {rows:,} lignes ({percentage:.1f}%)")
        
        print(f"\n Analyse terminée avec succès!")
        
    except Exception as e:
        print(f" Erreur lors de l'analyse: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
