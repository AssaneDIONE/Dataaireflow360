#!/usr/bin/env python3
"""
Script pour explorer et analyser le contenu des tables du Data Warehouse
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as spark_sum, avg, max as spark_max, min as spark_min

# Initialiser Spark
spark = SparkSession.builder \
    .appName("DataWarehouse_Explorer") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Chemin vers le warehouse
warehouse_path = "hdfs://localhost:9000/warehouse"

def analyze_table(table_name, df):
    """Analyse compl√®te d'une table"""
    print(f"\n{'='*60}")
    print(f"üìä ANALYSE DE LA TABLE : {table_name.upper()}")
    print(f"{'='*60}")
    
    # 1. Sch√©ma de la table
    print(f"\nüîç SCH√âMA DE LA TABLE :")
    df.printSchema()
    
    # 2. Nombre de lignes
    row_count = df.count()
    print(f"\nüìà STATISTIQUES G√âN√âRALES :")
    print(f"   ‚Ä¢ Nombre de lignes : {row_count:,}")
    print(f"   ‚Ä¢ Nombre de colonnes : {len(df.columns)}")
    
    # 3. Aper√ßu des donn√©es (5 premi√®res lignes)
    print(f"\nüëÄ APER√áU DES DONN√âES (5 premi√®res lignes) :")
    df.show(5, truncate=False)
    
    # 4. Statistiques descriptives pour les colonnes num√©riques
    numeric_cols = [field.name for field in df.schema.fields 
                   if field.dataType.typeName() in ['integer', 'double', 'float', 'long']]
    
    if numeric_cols:
        print(f"\nüìä STATISTIQUES NUM√âRIQUES :")
        df.select(numeric_cols).describe().show()
    
    # 5. Valeurs nulles par colonne
    print(f"\n‚ùå VALEURS NULLES PAR COLONNE :")
    null_counts = []
    for col_name in df.columns:
        null_count = df.filter(col(col_name).isNull()).count()
        null_percentage = (null_count / row_count * 100) if row_count > 0 else 0
        null_counts.append((col_name, null_count, f"{null_percentage:.2f}%"))
    
    for col_name, null_count, percentage in null_counts:
        print(f"   ‚Ä¢ {col_name}: {null_count:,} ({percentage})")
    
    # 6. Cardinalit√© (valeurs distinctes) pour colonnes importantes
    print(f"\nüî¢ CARDINALIT√â (valeurs distinctes) :")
    for col_name in df.columns:
        if df.schema[col_name].dataType.typeName() == 'string':
            distinct_count = df.select(col_name).distinct().count()
            print(f"   ‚Ä¢ {col_name}: {distinct_count:,} valeurs distinctes")
    
    return row_count

def analyze_dimension_relationships():
    """Analyse les relations entre les dimensions"""
    print(f"\n{'='*80}")
    print(f"üîó ANALYSE DES RELATIONS ENTRE TABLES")
    print(f"{'='*80}")
    
    # Charger les tables
    dim_passager = spark.read.parquet(f"{warehouse_path}/dim_passager")
    dim_transport = spark.read.parquet(f"{warehouse_path}/dim_transport") 
    dim_stop = spark.read.parquet(f"{warehouse_path}/dim_stop")
    dim_incident = spark.read.parquet(f"{warehouse_path}/dim_incident")
    fact_transport = spark.read.parquet(f"{warehouse_path}/fact_transport")
    
    print(f"\nüöó RELATIONS PAR VEHICLE_ID :")
    # Vehicles uniques dans chaque table
    vehicles_transport = dim_transport.select("vehicle_id").distinct().count()
    vehicles_passager = dim_passager.select("vehicle_id").distinct().count()
    vehicles_incident = dim_incident.select("vehicle_id").distinct().count()
    vehicles_fact = fact_transport.select("vehicle_id").distinct().count()
    
    print(f"   ‚Ä¢ Transport: {vehicles_transport:,} v√©hicules")
    print(f"   ‚Ä¢ Passagers: {vehicles_passager:,} v√©hicules") 
    print(f"   ‚Ä¢ Incidents: {vehicles_incident:,} v√©hicules")
    print(f"   ‚Ä¢ Table de faits: {vehicles_fact:,} v√©hicules")
    
    print(f"\nüöè RELATIONS PAR STOP_ID :")
    stops_dim = dim_stop.select("stop_id").distinct().count()
    stops_passager = dim_passager.select("stop_id").distinct().count()
    stops_fact = fact_transport.select("stop_id").distinct().count()
    
    print(f"   ‚Ä¢ Dimension stops: {stops_dim:,} arr√™ts")
    print(f"   ‚Ä¢ Dans passagers: {stops_passager:,} arr√™ts")
    print(f"   ‚Ä¢ Dans table de faits: {stops_fact:,} arr√™ts")

def analyze_business_metrics():
    """Analyse des m√©triques m√©tier importantes"""
    print(f"\n{'='*80}")
    print(f"üìà M√âTRIQUES M√âTIER CL√âS")
    print(f"{'='*80}")
    
    fact_transport = spark.read.parquet(f"{warehouse_path}/fact_transport")
    
    print(f"\nüöå ANALYSE DES TRANSPORTS :")
    # R√©partition par type de transport
    print("R√©partition par type de transport :")
    fact_transport.groupBy("type_transport").count().orderBy("count", ascending=False).show()
    
    # R√©partition par compagnie
    print("Top 10 des compagnies par nombre de v√©hicules :")
    fact_transport.groupBy("company_name").count().orderBy("count", ascending=False).show(10)
    
    # Statistiques de capacit√©
    print("Statistiques de capacit√© des v√©hicules :")
    fact_transport.select("capacity").describe().show()
    
    print(f"\n‚ö†Ô∏è  ANALYSE DES INCIDENTS :")
    # Incidents par v√©hicule
    incidents_stats = fact_transport.filter(col("incident_count").isNotNull()) \
        .agg(
            spark_sum("incident_count").alias("total_incidents"),
            avg("incident_count").alias("avg_incidents_per_vehicle"),
            spark_max("incident_count").alias("max_incidents"),
            spark_sum("total_delay_minutes").alias("total_delay"),
            avg("total_delay_minutes").alias("avg_delay_per_vehicle")
        ).collect()[0]
    
    print(f"   ‚Ä¢ Total incidents: {incidents_stats['total_incidents']}")
    print(f"   ‚Ä¢ Moyenne incidents/v√©hicule: {incidents_stats['avg_incidents_per_vehicle']:.2f}")
    print(f"   ‚Ä¢ Maximum incidents/v√©hicule: {incidents_stats['max_incidents']}")
    print(f"   ‚Ä¢ Retard total (minutes): {incidents_stats['total_delay']}")
    print(f"   ‚Ä¢ Retard moyen/v√©hicule: {incidents_stats['avg_delay_per_vehicle']:.2f} min")
    
    print(f"\nüë• ANALYSE DES PASSAGERS :")
    passenger_stats = fact_transport.filter(col("passenger_count").isNotNull()) \
        .agg(
            spark_sum("passenger_count").alias("total_passengers"),
            avg("passenger_count").alias("avg_passengers"),
            spark_max("passenger_count").alias("max_passengers")
        ).collect()[0]
    
    print(f"   ‚Ä¢ Total passagers: {passenger_stats['total_passengers']}")
    print(f"   ‚Ä¢ Moyenne passagers/v√©hicule: {passenger_stats['avg_passengers']:.2f}")
    print(f"   ‚Ä¢ Maximum passagers/v√©hicule: {passenger_stats['max_passengers']}")

def main():
    """Fonction principale d'exploration"""
    print("üèóÔ∏è  EXPLORATION COMPL√àTE DU DATA WAREHOUSE TRANSPORT")
    
    try:
        # Analyser chaque dimension
        tables_info = {}
        
        print(f"\nüéØ ANALYSE DES TABLES DE DIMENSIONS")
        print(f"{'='*50}")
        
        # Dimension Passager
        dim_passager = spark.read.parquet(f"{warehouse_path}/dim_passager")
        tables_info['dim_passager'] = analyze_table("DIM_PASSAGER", dim_passager)
        
        # Dimension Transport  
        dim_transport = spark.read.parquet(f"{warehouse_path}/dim_transport")
        tables_info['dim_transport'] = analyze_table("DIM_TRANSPORT", dim_transport)
        
        # Dimension Stop
        dim_stop = spark.read.parquet(f"{warehouse_path}/dim_stop") 
        tables_info['dim_stop'] = analyze_table("DIM_STOP", dim_stop)
        
        # Dimension Incident
        dim_incident = spark.read.parquet(f"{warehouse_path}/dim_incident")
        tables_info['dim_incident'] = analyze_table("DIM_INCIDENT", dim_incident)
        
        print(f"\nüéØ ANALYSE DE LA TABLE DE FAITS")
        print(f"{'='*40}")
        
        # Table de faits
        fact_transport = spark.read.parquet(f"{warehouse_path}/fact_transport")
        tables_info['fact_transport'] = analyze_table("FACT_TRANSPORT", fact_transport)
        
        # Analyses relationnelles
        analyze_dimension_relationships()
        
        # M√©triques m√©tier
        analyze_business_metrics()
        
        # R√©sum√© final
        print(f"\n{'='*80}")
        print(f"üìã R√âSUM√â EX√âCUTIF DU DATA WAREHOUSE")
        print(f"{'='*80}")
        
        total_rows = sum(tables_info.values())
        print(f"   üìä Total lignes dans le warehouse: {total_rows:,}")
        print(f"   üìä R√©partition par table:")
        for table, rows in tables_info.items():
            percentage = (rows / total_rows * 100) if total_rows > 0 else 0
            print(f"      ‚Ä¢ {table}: {rows:,} lignes ({percentage:.1f}%)")
        
        print(f"\n‚úÖ Analyse termin√©e avec succ√®s!")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()